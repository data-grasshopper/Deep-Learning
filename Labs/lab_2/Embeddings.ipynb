{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "We will use the embeddings through the whole lab. They are simply represented by a weight matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   1.   2.   3.]\n",
      " [  4.   5.   6.   7.]\n",
      " [  8.   9.  10.  11.]\n",
      " [ 12.  13.  14.  15.]\n",
      " [ 16.  17.  18.  19.]\n",
      " [ 20.  21.  22.  23.]\n",
      " [ 24.  25.  26.  27.]\n",
      " [ 28.  29.  30.  31.]\n",
      " [ 32.  33.  34.  35.]\n",
      " [ 36.  37.  38.  39.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_size = 4\n",
    "vocab_size = 10\n",
    "\n",
    "embedding = np.arange(embedding_size * vocab_size, dtype='float')\n",
    "embedding = embedding.reshape(vocab_size, embedding_size)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the embedding for a given symbol $i$, you may:\n",
    " - compute a one-hot encoding of $i$, then compute a dot product with the embedding matrix\n",
    " - simply index (slice) the embedding matrix by $i$, using numpy indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 12.  13.  14.  15.]\n",
      "[ 12.  13.  14.  15.]\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "onehot = np.zeros(10)\n",
    "onehot[i] = 1.\n",
    "item_embedding = np.dot(onehot, embedding)\n",
    "print(item_embedding)\n",
    "print(embedding[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In Keras, embeddings have an extra parameter, `input_length` which is typically used when having a sequence of symbols as input (think sequence of words). In our case, the length will always be 1.\n",
    "\n",
    "```py\n",
    "Embedding(output_dim=embedding_size, input_dim=vocab_size,\n",
    "          input_length=sequence_length, name='my_embedding')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, Flatten\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 1, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Input(shape=[1], name='input')\n",
    "emb = Embedding(output_dim=embedding_size, input_dim=vocab_size,\n",
    "          input_length=1, name='my_embedding')(x)\n",
    "model = Model(input=x,output=emb)\n",
    "model.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of an embedding layer is then a 3-d tensor of shape `(batch_size, sequence_length, embedding_size)`\n",
    "To remove the sequence dimension, useless in our case, we use the `Flatten()` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Input(shape=[1], name='input')\n",
    "emb = Embedding(output_dim=embedding_size, input_dim=vocab_size,\n",
    "          input_length=1, name='my_embedding')(x)\n",
    "\n",
    "# Add a flatten layer to remove useless \"sequence\" dimension\n",
    "y = Flatten()(emb)\n",
    "\n",
    "model = Model(input=x,output=y)\n",
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
