{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "We will use the embeddings through the whole lab. They are simply represented by a weight matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   1.   2.   3.]\n",
      " [  4.   5.   6.   7.]\n",
      " [  8.   9.  10.  11.]\n",
      " [ 12.  13.  14.  15.]\n",
      " [ 16.  17.  18.  19.]\n",
      " [ 20.  21.  22.  23.]\n",
      " [ 24.  25.  26.  27.]\n",
      " [ 28.  29.  30.  31.]\n",
      " [ 32.  33.  34.  35.]\n",
      " [ 36.  37.  38.  39.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_size = 4\n",
    "vocab_size = 10\n",
    "\n",
    "embedding = np.arange(embedding_size * vocab_size, dtype='float')\n",
    "embedding = embedding.reshape(vocab_size, embedding_size)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the embedding for a given symbol $i$, you may:\n",
    " - compute a one-hot encoding of $i$, then compute a dot product with the embedding matrix\n",
    " - simply index (slice) the embedding matrix by $i$, using numpy indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "onehot = np.zeros(10)\n",
    "onehot[i] = 1.\n",
    "onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 12.  13.  14.  15.]\n"
     ]
    }
   ],
   "source": [
    "embedding_vector = np.dot(onehot, embedding)\n",
    "print(embedding_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 12.  13.  14.  15.]\n"
     ]
    }
   ],
   "source": [
    "print(embedding[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### The Embedding layer in Keras\n",
    "\n",
    "In Keras, embeddings have an extra parameter, `input_length` which is typically used when having a sequence of symbols as input (think sequence of words). In our case, the length will always be 1.\n",
    "\n",
    "```py\n",
    "Embedding(output_dim=embedding_size, input_dim=vocab_size,\n",
    "          input_length=sequence_length, name='my_embedding')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    output_dim=embedding_size, input_dim=vocab_size,\n",
    "    input_length=1, name='my_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use it as part of a Keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 1, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "x = Input(shape=[1], name='input')\n",
    "embedding = embedding_layer(x)\n",
    "model = Model(input=x, output=embedding)\n",
    "model.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding weights are randomly initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.00010885,  0.04235527,  0.00520309,  0.0379991 ],\n",
       "        [-0.0275134 ,  0.04514914, -0.03284432, -0.02928448],\n",
       "        [ 0.0040774 , -0.01528351,  0.00097798,  0.00578408],\n",
       "        [-0.01080111,  0.03560133, -0.03600425,  0.00415856],\n",
       "        [-0.03910469,  0.04213334,  0.01893706, -0.0020257 ],\n",
       "        [ 0.03863158, -0.00525854,  0.03180087,  0.02923371],\n",
       "        [ 0.03271924, -0.02948186,  0.01590746, -0.02079001],\n",
       "        [-0.01088799, -0.04269426, -0.00206385,  0.00180041],\n",
       "        [ 0.0463028 ,  0.01722094,  0.02712139, -0.03782616],\n",
       "        [-0.00826054,  0.04071173,  0.00534217,  0.03200009]], dtype=float32)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00010885,  0.04235527,  0.00520309,  0.0379991 ]],\n",
       "\n",
       "       [[-0.01080111,  0.03560133, -0.03600425,  0.00415856]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[0],\n",
    "               [3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of an embedding layer is then a 3-d tensor of shape `(batch_size, sequence_length, embedding_size)`\n",
    "To remove the sequence dimension, useless in our case, we use the `Flatten()` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Flatten\n",
    "\n",
    "x = Input(shape=[1], name='input')\n",
    "\n",
    "# Add a flatten layer to remove useless \"sequence\" dimension\n",
    "y = Flatten()(embedding_layer(x))\n",
    "\n",
    "model2 = Model(input=x, output=y)\n",
    "model2.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00010885,  0.04235527,  0.00520309,  0.0379991 ],\n",
       "       [-0.01080111,  0.03560133, -0.03600425,  0.00415856]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict([[0],\n",
    "                [3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we re-used the same `embedding_layer` instance in both `model` and `model2`: therefore the two model share exactly the same weights in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
